{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "lJy5wsZuHvXz"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png\" srcset=\"https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png/w_130 130w, https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png/w_260 260w, https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png/w_390 390w, https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png/w_520 520w, https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png/w_650 650w, https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png/w_780 780w, https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png/w_910 910w, https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png/w_1040 1040w, https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png/w_1170 1170w, https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png/w_1290 1290w\" sizes=\"100vw\" width=\"1290\">\n",
        "<p style='margin-top: 1rem; margin-bottom: 1rem;'>Developed by Marco Frodl, Principal Consultant for Generative AI @ <a href='https://go.mfr.one/tt-en' _target='blank'>Thinktecture AG</a> -- More about me on my <a href='https://go.mfr.one/marcofrodl-en' _target='blank'>profile page</a></p>"
      ],
      "metadata": {
        "id": "Vigmlask6ys0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prettify Colab Notebook outputs"
      ],
      "metadata": {
        "id": "lJy5wsZuHvXz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import HTML, display\n",
        "\n",
        "def set_css():\n",
        "  display(HTML('''\n",
        "  <style>\n",
        "    pre {\n",
        "        white-space: pre-wrap;\n",
        "    }\n",
        "  </style>\n",
        "  '''))\n",
        "get_ipython().events.register('pre_run_cell', set_css)"
      ],
      "metadata": {
        "id": "xOA06-GRFTeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set API keys via Colab Secrets"
      ],
      "metadata": {
        "id": "XUC6k6SzJE5a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import Colab Secrets userdata module\n",
        "from google.colab import userdata\n",
        "\n",
        "# set OpenAI API key\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# set Langfuse API keys\n",
        "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = userdata.get('LANGFUSE_PUBLIC_KEY')\n",
        "os.environ[\"LANGFUSE_SECRET_KEY\"] = userdata.get('LANGFUSE_SECRET_KEY')"
      ],
      "metadata": {
        "id": "7Aj0mSvhyJI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load libraries"
      ],
      "metadata": {
        "id": "6jh11ajrHjFC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install protobuf==3.20.3 langchain==0.3.1 langchain-openai==0.2.1 langchain-mistralai==0.2.0 langfuse==2.51.2"
      ],
      "metadata": {
        "id": "pfrfCqwVVG_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# let's check the imported versions of the most important libraries\n",
        "!pip show langchain\n",
        "!pip show langchain-openai\n",
        "!pip show langchain-mistralai\n",
        "!pip show langfuse"
      ],
      "metadata": {
        "id": "wsfvcpLnQyw_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Langfuse"
      ],
      "metadata": {
        "id": "vOP866IZG87z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare Langfuse as debugging and tracing framework for our Generative AI application - never develop GenAI apps without that!\n",
        "from langfuse.callback import CallbackHandler\n",
        "handler = CallbackHandler()"
      ],
      "metadata": {
        "id": "gdG06ivx5xV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare LLM"
      ],
      "metadata": {
        "id": "PflymEKFgBBl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OpenAI LLM"
      ],
      "metadata": {
        "id": "DaIaBsel8Cf-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "llm_openai = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, max_tokens=1000) # gpt-4o-mini, gpt-4o"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "CQCcOM-S8Jmp",
        "outputId": "0ea4f35f-801c-437f-af3a-cae39a7f5a41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mistral AI LLM"
      ],
      "metadata": {
        "id": "QZilCIWJ7_PX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_mistralai import ChatMistralAI\n",
        "llm_mistralai = ChatMistralAI(mistral_api_key=userdata.get('MISTRAL_API_KEY'), temperature=0, max_tokens=1000, model=\"open-mistral-nemo-2407\") # open-mistral-nemo-2407, mistral-large-2407, mistral-small-2409"
      ],
      "metadata": {
        "id": "YJ3o8h7OgBBm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "30bbd4dc-ef34-4de1-82cc-72bbe73b53dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <style>\n",
              "    pre {\n",
              "        white-space: pre-wrap;\n",
              "    }\n",
              "  </style>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple LLM chat call"
      ],
      "metadata": {
        "id": "rn4nosl8fMCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pure as water\n",
        "result = llm_openai.invoke(\"Which is the best generative ai large language model in the world?\")\n",
        "print(\"--- OpenAI ---\")\n",
        "print(result.content)\n",
        "print(result.response_metadata)\n",
        "\n",
        "result = llm_mistralai.invoke(\"Which is the best generative ai large language model in the world?\")\n",
        "print(\"--- MistralAI ---\")\n",
        "print(result.content)\n",
        "print(result.response_metadata)\n"
      ],
      "metadata": {
        "id": "cXO6J-UJxedm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat with Metadata"
      ],
      "metadata": {
        "id": "tFi-6yoeCu6D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# little bit more context about who is talking\n",
        "from langchain_core.messages import HumanMessage\n",
        "result = llm_openai.invoke([HumanMessage(content=\"Kennst du meinen Namen?\", name=\"Marco\")])\n",
        "print(\"--- OpenAI ---\")\n",
        "print(result.content)\n",
        "print(result.response_metadata)\n",
        "\n",
        "result = llm_mistralai.invoke([HumanMessage(content=\"Kennst du meinen Namen?\", name=\"Marco\")]) # name is not supported by ChatMistralAI\n",
        "print(\"--- MistralAI ---\")\n",
        "print(result.content)\n",
        "print(result.response_metadata)"
      ],
      "metadata": {
        "id": "xO8T25nu_uEj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat with Tracing"
      ],
      "metadata": {
        "id": "V1noY3GTC0_W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# lets optimize and trace whats happening\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "messages = [SystemMessage(content=\"Du bist eine AI, die den User Duzt und gerne Wörter wie Bro, Digga und ich schwör verwendet. Der Name des Users ist Marco\")]\n",
        "messages.extend([HumanMessage(content=\"Kennst du meinen Namen?\")])\n",
        "\n",
        "result = llm_openai.invoke(messages,{\"callbacks\":[handler]})\n",
        "print(\"--- OpenAI ---\")\n",
        "print(result.content)\n",
        "print(result.response_metadata)\n",
        "\n",
        "result = llm_mistralai.invoke(messages,{\"callbacks\":[handler]})\n",
        "print(\"--- MistralAI ---\")\n",
        "print(result.content)\n",
        "print(result.response_metadata)"
      ],
      "metadata": {
        "id": "-rkMVTf3BoSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chat with history"
      ],
      "metadata": {
        "id": "IKrY_r86C5_9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dialog based chat\n",
        "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
        "messages = [\n",
        "    SystemMessage(content=\"Du bist eine AI, die sich mit Popmusik auskennt und viele Fakten in Antworten einfliessen lässt. Du Duzt den User.\"),\n",
        "    HumanMessage(content=\"Genesis war ohne Peter Gabriel ein Schatten seiner selbt. Wer war danach der Sänger?\"),\n",
        "    AIMessage(content=\"Phil Collins - allerdings muss ich beim Thema Erfolg widersprechen!\"),\n",
        "    HumanMessage(content=\"Ok, warum?\")]\n",
        "\n",
        "result = llm_openai.invoke(messages,{\"callbacks\":[handler]})\n",
        "print(\"--- OpenAI ---\")\n",
        "print(result.content)\n",
        "print(result.response_metadata)\n",
        "\n",
        "result = llm_mistralai.invoke(messages,{\"callbacks\":[handler]})\n",
        "print(\"--- MistralAI ---\")\n",
        "print(result.content)\n",
        "print(result.response_metadata)"
      ],
      "metadata": {
        "id": "iYCtPi4YDyR_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}