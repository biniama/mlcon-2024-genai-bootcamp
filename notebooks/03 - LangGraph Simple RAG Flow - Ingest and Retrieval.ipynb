{"cells":[{"cell_type":"markdown","metadata":{"id":"Vigmlask6ys0"},"source":["<img src=\"https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png\" srcset=\"https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png/w_130 130w, https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png/w_260 260w, https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png/w_390 390w, https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png/w_520 520w, https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png/w_650 650w, https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png/w_780 780w, https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png/w_910 910w, https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png/w_1040 1040w, https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png/w_1170 1170w, https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png/w_1290 1290w\" sizes=\"100vw\" width=\"1290\">\n","<p style='margin-top: 1rem; margin-bottom: 1rem;'>Developed by Marco Frodl, Principal Consultant for Generative AI @ <a href='https://go.mfr.one/tt-en' _target='blank'>Thinktecture AG</a> -- More about me on my <a href='https://go.mfr.one/marcofrodl-en' _target='blank'>profile page</a></p>"]},{"cell_type":"markdown","metadata":{"id":"lJy5wsZuHvXz"},"source":["## Prettify Colab Notebook outputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xOA06-GRFTeK"},"outputs":[],"source":["from IPython.display import HTML, display\n","\n","def set_css():\n","  display(HTML('''\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  '''))\n","get_ipython().events.register('pre_run_cell', set_css)"]},{"cell_type":"markdown","metadata":{"id":"XUC6k6SzJE5a"},"source":["## Set API keys via Colab Secrets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Aj0mSvhyJI5"},"outputs":[],"source":["# import Colab Secrets userdata module\n","from google.colab import userdata\n","\n","# set LLM Api Key\n","import os\n","os.environ[\"OPENAI_API_KEY\"] = userdata.get('OVH_AI_ENDPOINTS_ACCESS_TOKEN')\n","\n","# set Langfuse API keys\n","os.environ[\"LANGFUSE_PUBLIC_KEY\"] = userdata.get('LANGFUSE_PUBLIC_KEY')\n","os.environ[\"LANGFUSE_SECRET_KEY\"] = userdata.get('LANGFUSE_SECRET_KEY')"]},{"cell_type":"markdown","metadata":{"id":"6jh11ajrHjFC"},"source":["## Load libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pfrfCqwVVG_Y"},"outputs":[],"source":["!pip -q install langchain==0.3.1 langchain-core==0.3.7 langchain-community==0.3.1 langchain-openai==0.2.1 langchain-qdrant==0.1.4\n","!pip -q install langfuse==2.53.3\n","!pip -q install langgraph==0.2.32\n","!pip -q install feedparser"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wsfvcpLnQyw_"},"outputs":[],"source":["# let's check the imported versions of the most important libraries\n","!pip show langchain\n","!pip show langchain-core\n","!pip show langchain_community\n","!pip show langchain_mistralai\n","!pip show langchain-qdrant\n","!pip show langgraph"]},{"cell_type":"markdown","source":["## Prepare Langfuse"],"metadata":{"id":"xwTHdClPfjCk"}},{"cell_type":"code","source":["# prepare Langfuse as debugging and tracing framework for our Generative AI application - never develop GenAI apps without that!\n","from langfuse.callback import CallbackHandler\n","from langfuse import Langfuse\n","\n","# Initialize CallbackHandler\n","lf_handler = CallbackHandler()\n","\n","# Initialize Langfuse\n","langfuse = Langfuse()"],"metadata":{"id":"w-03oZgJfjq7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Prepare LLM"],"metadata":{"id":"my_ya__kiD6A"}},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","\n","TEMPERATURE = 0\n","MAX_TOKENS = 1500\n","\n","# OVH AI Endpoints Overview: https://endpoints.ai.cloud.ovh.net/\n","\n","# OVH Mistral-7B-0.2\n","#MODEL_NAME = \"Mistral-7B-Instruct-v0.2\"\n","#BASE_URL = \"https://mistral-7b-instruct-v02.endpoints.kepler.ai.cloud.ovh.net/api/openai_compat/v1\"\n","\n","# OVH Mixtral 8x22B\n","#MODEL_NAME = \"Mixtral-8x22B-Instruct-v0.1\"\n","#BASE_URL = \"https://mixtral-8x22b-instruct-v01.endpoints.kepler.ai.cloud.ovh.net/api/openai_compat/v1\"\n","\n","# OVH Llama3-70B\n","#MODEL_NAME = \"Meta-Llama-3-70B-Instruct\"\n","#BASE_URL = \"https://llama-3-70b-instruct.endpoints.kepler.ai.cloud.ovh.net/api/openai_compat/v1\"\n","\n","# OVH Llama3-8B\n","#MODEL_NAME = \"Meta-Llama-3-8B-Instruct\"\n","#BASE_URL = \"https://llama-3-8b-instruct.endpoints.kepler.ai.cloud.ovh.net/api/openai_compat/v1\"\n","\n","# OVH Llama3.1-70B\n","MODEL_NAME = \"Meta-Llama-3_1-70B-Instruct\"\n","BASE_URL = \"https://llama-3-1-70b-instruct.endpoints.kepler.ai.cloud.ovh.net/api/openai_compat/v1\"\n","\n","# OVH Mamba Codestral 7B\n","#MODEL_NAME = \"mamba-codestral-7B-v0.1\"\n","#BASE_URL = \"https://mamba-codestral-7b-v0-1.endpoints.kepler.ai.cloud.ovh.net/api/openai_compat/v1/\"\n","\n","# OVH CodeLlama-13B\n","#MODEL_NAME = \"CodeLlama-13b-Instruct-hf\"\n","#BASE_URL = \"https://codellama-13b-instruct-hf.endpoints.kepler.ai.cloud.ovh.net/api/openai_compat/v1\"\n","\n","llm = ChatOpenAI(model=MODEL_NAME, temperature=TEMPERATURE, max_tokens=MAX_TOKENS, base_url=BASE_URL)\n","\n","# https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html\n","llm_json_mode = llm.bind(response_format={\"type\": \"json_object\"})"],"metadata":{"id":"2vBVfklTiEfs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KUUx73p8Iz6Y"},"source":["## Prepare Embedding Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XCuM-mHAIz6Y"},"outputs":[],"source":["# using embeddings from MistralAI oder OpenAI is super easy with LangChain - but you would need a paid api key\n","#from langchain_mistralai import MistralAIEmbeddings\n","#embeddings = MistralAIEmbeddings(mistral_api_key=userdata.get('MISTRAL_API_KEY'))\n","\n","# we want to use free OVH AI endpoints. Therefore we use the LangChain base implementation for embeddings and create a small wrapper for OVH\n","# it's more complex than Mistral or OpenAI but free\n","from langchain_core.embeddings import Embeddings\n","import requests\n","\n","class OVHEmbeddings(Embeddings):\n","    def __init__(self, api_url: str, access_token: str):\n","        self.api_url = api_url\n","        self.headers = {\n","            \"Content-Type\": \"text/plain\",\n","            \"Authorization\": f\"Bearer {access_token}\",\n","        }\n","\n","    def embed_query(self, text: str) -> list:\n","        \"\"\"Generate embeddings for a single query.\"\"\"\n","        response = requests.post(self.api_url, data=text, headers=self.headers)\n","        if response.status_code == 200:\n","            return response.json()  # Ensure this returns a list of floats\n","        else:\n","            raise ValueError(f\"Error from embedding API: {response.status_code}, {response.text}\")\n","\n","    def embed_documents(self, texts: list) -> list:\n","        \"\"\"Generate embeddings for a list of documents.\"\"\"\n","        # Check if the API supports batch processing\n","        # Example:\n","        # response = requests.post(self.api_url, json={\"texts\": texts}, headers=self.headers)\n","        # Ensure proper handling based on the API's documentation\n","\n","        # Fallback to single calls if batching is not supported\n","        embeddings = []\n","        for text in texts:\n","            embeddings.append(self.embed_query(text))\n","        return embeddings\n","\n","# Initialize the embedding model\n","api_url = \"https://bge-m3.endpoints.kepler.ai.cloud.ovh.net/api/text2vec\"\n","\n","embed_model = OVHEmbeddings(api_url, os.environ.get(\"OPENAI_API_KEY\"))"]},{"cell_type":"markdown","metadata":{"id":"xbuD7l0FvrgM"},"source":["## Setup Ingestion Graph"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ywyPa2HLvueZ"},"outputs":[],"source":["import feedparser\n","from typing_extensions import TypedDict\n","from typing import List\n","from langchain_core.documents import Document\n","\n","store_wiki = None\n","\n","class GraphState(TypedDict):\n","    \"\"\"\n","    Graph state is a dictionary that contains information we want to propagate to, and modify in, each graph node.\n","    \"\"\"\n","    generation : str # LLM generation\n","    documents : List[str] # List of retrieved documents\n","\n","\n","from langgraph.graph import START, END\n","\n","### Nodes\n","def rssloader(state):\n","    \"\"\"\n","    loads the rss feed with wiki content and creates langchain documents from it\n","\n","    Args:\n","        state (dict): The current graph state\n","\n","    Returns:\n","        state (dict): New key added to state, documents, that contains loaded documents from rss feed\n","    \"\"\"\n","    # Parse the RSS feed\n","    feed_url = \"https://demowiki.webstage.work/category/angular/feed/\"\n","    feed = feedparser.parse(feed_url)\n","    results = feed.entries\n","\n","    # List to store the generated Document objects\n","    documents = []\n","\n","    # Iterate over each entry in the feed\n","    for entry in results:\n","        # Extract the page content\n","        if 'content' in entry and entry.content:\n","            page_content = entry.content[0]['value']\n","        else:\n","            page_content = entry.get('summary', '')  # Fallback to summary if no content is present\n","\n","        # Extract metadata\n","        metadata = {\n","            \"title\": entry.get('title', 'No Title'),\n","            \"link\": entry.get('link', 'No Link'),\n","            \"author\": entry.get('author', 'Unknown Author'),\n","            \"publish_date\": entry.get('published', 'No Date'),\n","            \"feed\": feed_url\n","        }\n","\n","        # Create a Document object for this entry\n","        document = Document(page_content=page_content, metadata=metadata)\n","\n","        # Append the document to the list\n","        documents.append(document)\n","    return {\"documents\": documents}\n","\n","def reindex(state):\n","    global store_wiki\n","    \"\"\"\n","    re-builds Qdrant vectorstore for wiki content\n","\n","    Args:\n","        state (dict): The current graph state\n","\n","    Returns:\n","        state (dict): New key added to state, generation, containing a finish message\n","    \"\"\"\n","\n","    # get documents from state\n","    documents = state[\"documents\"]\n","\n","    # Re-build Qdrant vectore store\n","    from langchain_qdrant import QdrantVectorStore\n","\n","    store_wiki = QdrantVectorStore.from_documents(\n","      documents,\n","      embed_model,\n","      location=\":memory:\",  # Local mode with in-memory storage only just for the demo\n","      collection_name=\"wiki\",\n","      force_recreate=True,\n","    )\n","\n","    return {\"generation\": documents}\n","\n","from langgraph.graph import StateGraph\n","workflow = StateGraph(GraphState)\n","\n","# Define the nodes\n","workflow.add_node(\"rssloader\", rssloader) # rssloader\n","workflow.add_node(\"reindex\", reindex) # reindex\n","\n","# Define the edges\n","workflow.add_edge(START, \"rssloader\")\n","workflow.add_edge(\"rssloader\", \"reindex\")\n","workflow.add_edge(\"reindex\", END)\n","\n","# Compile\n","graph = workflow.compile()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K9I2Ugcz4sTq"},"outputs":[],"source":["# view graph\n","from IPython.display import Image, display\n","display(Image(graph.get_graph().draw_mermaid_png()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xR_AMAlk1BUQ"},"outputs":[],"source":["# run graph\n","result = graph.invoke({\"generation\":\"\"})\n","print(result)"]},{"cell_type":"markdown","metadata":{"id":"rgU1L1Z2e4gP"},"source":["## Attach Vector Stores as Retrievers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6vqlkVBwe4gQ"},"outputs":[],"source":["# create retriever\n","wiki_retriever = store_wiki.as_retriever(search_kwargs={\"k\":1,})"]},{"cell_type":"markdown","metadata":{"id":"rDKarTdMe4gQ"},"source":["### Similarity search w/o score"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sGzF62uSe4gQ"},"outputs":[],"source":["query = \"Was ist besser signal inputs oder inputs()?\"\n","found_docs = store_wiki.similarity_search(query)\n","print(found_docs[0].page_content)\n","print(\"-\" *10)\n","print(found_docs[1].page_content)"]},{"cell_type":"markdown","metadata":{"id":"qcz4ecxfe4gQ"},"source":["## Simple Retrieval QA"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SJ5VeynEe4gR"},"outputs":[],"source":["# Define prompt for RAG\n","# we can define multiple prompts and use one prompt per data source/retriever\n","prompt_template = \"\"\"You are an assistant for question-answering tasks.\n","Think carefully about the context.\n","Just say 'Diese Frage kann ich nicht beantworten' if there is not enough or no context given.\n","Provide an answer to the user question using only the given context.\n","Use three sentences maximum and keep the answer concise.\n","Here is the context to use to answer the question:\n","\n","{context}\n","\n","Now, review the user question:\n","\n","{question}\n","\n","Write the answer in German. Don't output an English translation.\n","\n","Answer:\"\"\""]},{"cell_type":"markdown","metadata":{"id":"qdzhe1P9e4gR"},"source":["## Setup Retrieval Graph"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cHKt0OA3e4gR"},"outputs":[],"source":["from typing_extensions import TypedDict\n","from typing import List\n","\n","class GraphState(TypedDict):\n","    \"\"\"\n","    Graph state is a dictionary that contains information we want to propagate to, and modify in, each graph node.\n","    \"\"\"\n","    question : str # User question\n","    generation : str # LLM generation\n","    documents : List[str] # List of retrieved documents"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FCnalLOce4gR"},"outputs":[],"source":["from langchain_core.messages import HumanMessage\n","from langgraph.graph import START, END\n","\n","### Helper function\n","# Post-processing\n","def format_docs(docs):\n","    return \"\\n\\n\".join(doc.page_content for doc in docs)\n","\n","### Nodes\n","def retrieve(state):\n","    \"\"\"\n","    Retrieve documents from vectorstore\n","\n","    Args:\n","        state (dict): The current graph state\n","\n","    Returns:\n","        state (dict): New key added to state, documents, that contains retrieved documents\n","    \"\"\"\n","    print(\"---RETRIEVE---\")\n","    question = state[\"question\"]\n","\n","    # Write retrieved documents to documents key in state\n","    documents = wiki_retriever.invoke(question)\n","    return {\"documents\": documents}\n","\n","def generate(state):\n","    global llm\n","    \"\"\"\n","    Generate answer using RAG on retrieved documents\n","\n","    Args:\n","        state (dict): The current graph state\n","\n","    Returns:\n","        state (dict): New key added to state, generation, that contains LLM generation\n","    \"\"\"\n","    print(\"---GENERATE---\")\n","    question = state[\"question\"]\n","    documents = state[\"documents\"]\n","\n","    # RAG generation\n","    docs_txt = format_docs(documents)\n","    rag_prompt_formatted = prompt_template.format(context=docs_txt, question=question)\n","    generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])\n","    return {\"generation\": generation}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xeLe3kWAe4gS"},"outputs":[],"source":["from langgraph.graph import StateGraph\n","from IPython.display import Image, display\n","\n","workflow = StateGraph(GraphState)\n","\n","# Define the nodes\n","workflow.add_node(\"retrieve\", retrieve) # retrieve\n","workflow.add_node(\"generate\", generate) # generate\n","\n","# Define the edges\n","workflow.add_edge(START, \"retrieve\")\n","workflow.add_edge(\"retrieve\", \"generate\")\n","workflow.add_edge(\"generate\", END)\n","\n","# Compile\n","graph = workflow.compile()\n","display(Image(graph.get_graph().draw_mermaid_png()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9xcY2QHTe4gS"},"outputs":[],"source":["result = graph.invoke({\"question\": \"Was ist besser signal inputs oder inputs()?\"}, config={\"callbacks\": [lf_handler]})\n","print(result)\n","print(\"-\" * 10)\n","print(result[\"generation\"].content)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uIN6fZTTe4gS"},"outputs":[],"source":["inputs = {\"question\": \"Wie backe ich einen Apfelkuchen?\"}\n","for event in graph.stream(inputs, stream_mode=\"values\", config={\"callbacks\": [lf_handler]}):\n","    print(event)"]}],"metadata":{"colab":{"provenance":[{"file_id":"1rNhKKNIKoJ4m57_NrkacRzgeb8A36adI","timestamp":1732283828225},{"file_id":"1RJY2Iw1syFXQqskaW-trGSAK6TDw49os","timestamp":1727881968219},{"file_id":"1YW8tGacff05Ie7sSAtF7f6ItjA-l0R-B","timestamp":1727765579943},{"file_id":"14hP4Ld97QlJdWTBaHU2I2e0ZzMUw-WF-","timestamp":1727599529874},{"file_id":"1D2QriJvCzNUdU__n7VEWxebiYVnF1GwO","timestamp":1704649306386}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}