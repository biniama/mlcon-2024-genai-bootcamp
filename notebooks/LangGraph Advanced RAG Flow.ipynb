{"cells":[{"cell_type":"markdown","metadata":{"id":"Vigmlask6ys0"},"source":["<img src=\"https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png\" srcset=\"https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png/w_130 130w, https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png/w_260 260w, https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png/w_390 390w, https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png/w_520 520w, https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png/w_650 650w, https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png/w_780 780w, https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png/w_910 910w, https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png/w_1040 1040w, https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png/w_1170 1170w, https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png/w_1290 1290w\" sizes=\"100vw\" width=\"1290\">\n","<p style='margin-top: 1rem; margin-bottom: 1rem;'>Developed by Marco Frodl, Principal Consultant for Generative AI @ <a href='https://go.mfr.one/tt-en' _target='blank'>Thinktecture AG</a> -- More about me on my <a href='https://go.mfr.one/marcofrodl-en' _target='blank'>profile page</a></p>"]},{"cell_type":"markdown","metadata":{"id":"lJy5wsZuHvXz"},"source":["## Prettify Colab Notebook outputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xOA06-GRFTeK"},"outputs":[],"source":["from IPython.display import HTML, display\n","\n","def set_css():\n","  display(HTML('''\n","  <style>\n","    pre {\n","        white-space: pre-wrap;\n","    }\n","  </style>\n","  '''))\n","get_ipython().events.register('pre_run_cell', set_css)"]},{"cell_type":"markdown","metadata":{"id":"XUC6k6SzJE5a"},"source":["## Set API keys via Colab Secrets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Aj0mSvhyJI5"},"outputs":[],"source":["# import Colab Secrets userdata module\n","from google.colab import userdata\n","\n","# set LLM Api Key\n","import os\n","os.environ[\"OPENAI_API_KEY\"] = userdata.get('OVH_AI_ENDPOINTS_ACCESS_TOKEN')\n","\n","# set Langfuse API keys\n","os.environ[\"LANGFUSE_PUBLIC_KEY\"] = userdata.get('LANGFUSE_PUBLIC_KEY')\n","os.environ[\"LANGFUSE_SECRET_KEY\"] = userdata.get('LANGFUSE_SECRET_KEY')"]},{"cell_type":"markdown","metadata":{"id":"6jh11ajrHjFC"},"source":["## Load libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pfrfCqwVVG_Y"},"outputs":[],"source":["!pip -q install protobuf==3.20.3 langchain==0.3.1 langchain-core==0.3.7 langchain-openai==0.2.1 langchain-community==0.3.1 langchain-mistralai==0.2.0 langchain-qdrant==0.1.4\n","!pip -q install langgraph==0.2.32\n","!pip -q install langfuse==2.51.2\n","!pip -q install tavily-python"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wsfvcpLnQyw_"},"outputs":[],"source":["# let's check the imported versions of the most important libraries\n","!pip show langchain\n","!pip show langchain-core\n","!pip show langchain_openai\n","!pip show langchain_community\n","!pip show langchain_mistralai\n","!pip show langchain-qdrant\n","!pip show langgraph\n","!pip show langfuse"]},{"cell_type":"markdown","metadata":{"id":"PflymEKFgBBl"},"source":["## Prepare LLM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YJ3o8h7OgBBm"},"outputs":[],"source":["from langchain_openai import ChatOpenAI\n","\n","TEMPERATURE = 0\n","MAX_TOKENS = 1500\n","\n","# OVH AI Endpoints Overview: https://endpoints.ai.cloud.ovh.net/\n","\n","# OVH Mistral-7B-0.2\n","#MODEL_NAME = \"Mistral-7B-Instruct-v0.2\"\n","#BASE_URL = \"https://mistral-7b-instruct-v02.endpoints.kepler.ai.cloud.ovh.net/api/openai_compat/v1\"\n","\n","# OVH Mixtral 8x22B\n","#MODEL_NAME = \"Mixtral-8x22B-Instruct-v0.1\"\n","#BASE_URL = \"https://mixtral-8x22b-instruct-v01.endpoints.kepler.ai.cloud.ovh.net/api/openai_compat/v1\"\n","\n","# OVH Llama3-70B\n","#MODEL_NAME = \"Meta-Llama-3-70B-Instruct\"\n","#BASE_URL = \"https://llama-3-70b-instruct.endpoints.kepler.ai.cloud.ovh.net/api/openai_compat/v1\"\n","\n","# OVH Llama3-8B\n","#MODEL_NAME = \"Meta-Llama-3-8B-Instruct\"\n","#BASE_URL = \"https://llama-3-8b-instruct.endpoints.kepler.ai.cloud.ovh.net/api/openai_compat/v1\"\n","\n","# OVH Llama3.1-70B\n","MODEL_NAME = \"Meta-Llama-3_1-70B-Instruct\"\n","BASE_URL = \"https://llama-3-1-70b-instruct.endpoints.kepler.ai.cloud.ovh.net/api/openai_compat/v1\"\n","\n","# OVH Mamba Codestral 7B\n","#MODEL_NAME = \"mamba-codestral-7B-v0.1\"\n","#BASE_URL = \"https://mamba-codestral-7b-v0-1.endpoints.kepler.ai.cloud.ovh.net/api/openai_compat/v1/\"\n","\n","# OVH CodeLlama-13B\n","#MODEL_NAME = \"CodeLlama-13b-Instruct-hf\"\n","#BASE_URL = \"https://codellama-13b-instruct-hf.endpoints.kepler.ai.cloud.ovh.net/api/openai_compat/v1\"\n","\n","llm = ChatOpenAI(model=MODEL_NAME, temperature=TEMPERATURE, max_tokens=MAX_TOKENS, base_url=BASE_URL)\n","\n","# https://python.langchain.com/api_reference/openai/chat_models/langchain_openai.chat_models.base.ChatOpenAI.html\n","llm_json_mode = llm.bind(response_format={\"type\": \"json_object\"})"]},{"cell_type":"markdown","metadata":{"id":"KUUx73p8Iz6Y"},"source":["## Prepare Embedding Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XCuM-mHAIz6Y"},"outputs":[],"source":["# using embeddings from MistralAI oder OpenAI is super easy with LangChain - but you would need a paid api key\n","#from langchain_mistralai import MistralAIEmbeddings\n","#embeddings = MistralAIEmbeddings(mistral_api_key=userdata.get('MISTRAL_API_KEY'))\n","\n","# we want to use free OVH AI endpoints. Therefore we use the LangChain base implementation for embeddings and create a small wrapper for OVH\n","# it's more complex than Mistral or OpenAI but free\n","from langchain_core.embeddings import Embeddings\n","import requests\n","\n","class OVHEmbeddings(Embeddings):\n","    def __init__(self, api_url: str, access_token: str):\n","        self.api_url = api_url\n","        self.headers = {\n","            \"Content-Type\": \"text/plain\",\n","            \"Authorization\": f\"Bearer {access_token}\",\n","        }\n","\n","    def embed_query(self, text: str) -> list:\n","        \"\"\"Generate embeddings for a single query.\"\"\"\n","        response = requests.post(self.api_url, data=text, headers=self.headers)\n","        if response.status_code == 200:\n","            return response.json()  # Ensure this returns a list of floats\n","        else:\n","            raise ValueError(f\"Error from embedding API: {response.status_code}, {response.text}\")\n","\n","    def embed_documents(self, texts: list) -> list:\n","        \"\"\"Generate embeddings for a list of documents.\"\"\"\n","        # Check if the API supports batch processing\n","        # Example:\n","        # response = requests.post(self.api_url, json={\"texts\": texts}, headers=self.headers)\n","        # Ensure proper handling based on the API's documentation\n","\n","        # Fallback to single calls if batching is not supported\n","        embeddings = []\n","        for text in texts:\n","            embeddings.append(self.embed_query(text))\n","        return embeddings\n","\n","# Initialize the embedding model\n","api_url = \"https://bge-m3.endpoints.kepler.ai.cloud.ovh.net/api/text2vec\"\n","\n","embed_model = OVHEmbeddings(api_url, os.environ.get(\"OPENAI_API_KEY\"))"]},{"cell_type":"markdown","metadata":{"id":"vOP866IZG87z"},"source":["## Prepare LangFuse"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gdG06ivx5xV1"},"outputs":[],"source":["# prepare Langfuse as debugging and tracing framework for our Generative AI application - never develop GenAI apps without that!\n","from langfuse.callback import CallbackHandler\n","handler = CallbackHandler()"]},{"cell_type":"markdown","metadata":{"id":"D_q17SDWHaSq"},"source":["## Attach Vector Stores as Retrievers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8YVJ1rY8HeQK"},"outputs":[],"source":["# let's attach our Qdrant Vector store\n","from langchain_qdrant import QdrantVectorStore\n","\n","    store_wiki = QdrantVectorStore.from_documents(\n","      documents,\n","      embed_model,\n","      location=\":memory:\",  # Local mode with in-memory storage only just for the demo\n","      collection_name=\"wiki\",\n","      force_recreate=True,\n","    )\n","\n","store_wiki = QdrantVectorStore.from_existing_collection(\n","    url = userdata.get('QDRANT_INSTANCE_URL'),\n","    api_key = userdata.get('QDRANT_API_KEY'),\n","    collection_name = \"wiki\",\n","    embedding = embed_model,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uPJ4Ez7_BDT4"},"outputs":[],"source":["# create retriever\n","wiki_retriever = store_wiki.as_retriever(search_kwargs={\"k\":1,})"]},{"cell_type":"markdown","metadata":{"id":"aVMRD664Vnyj"},"source":["## Prepare prompts and instructions"]},{"cell_type":"markdown","metadata":{"id":"QK9KxjO7O6sC"},"source":["### Prompt for final answer generation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zGoTLGhqbgPa"},"outputs":[],"source":["# Define prompt for answer generation\n","prompt_template = \"\"\"You are an assistant for question-answering tasks at ACME GmbH.\n","Think carefully about the context.\n","Just say 'Diese Frage kann ich nicht beantworten' if there is not enough or no context given.\n","Provide an answer to the user question using only the given context.\n","Use three sentences maximum and keep the answer concise.\n","If the context mentions ACME guidelines, try to include it in the answer.\n","Here is the context to use to answer the question:\n","\n","{context}\n","\n","Now, review the user question:\n","\n","{question}\n","\n","Write the answer in German. Don't output an English translation.\n","\n","Answer:\"\"\""]},{"cell_type":"markdown","metadata":{"id":"1wuZ3mpzO-zo"},"source":["### Prompt for topic routing at workflow start"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5xLVYY8sVr6d"},"outputs":[],"source":["# define prompt for routing\n","import json\n","from langchain_core.messages import HumanMessage, SystemMessage\n","\n","router_instructions = \"\"\"You are an expert at routing a user question to a vectorstore or websearch.\n","\n","The vectorstore contains documents related to coding, programming, development practices, single page applications, the Angular framework, and coding guidelines for the company ACME.\n","\n","Use the vectorstore for any questions containing coding terms, code snippets, programming languages, or technologies relevant to development practices (even in other languages like German).\n","\n","If the question is related to coding or development but not specifically covered by the vectorstore, still return 'vectorstore'. Use 'websearch' for non-coding questions.\n","\n","Return JSON with a single key, \"datasource,\" that is 'websearch' or 'vectorstore' depending on the question.\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"madeIw49Mf8a"},"outputs":[],"source":["# test the routing prompt with multiple questions\n","questions = [\n","    \"Was ist besser signal inputs oder inputs()?\",\n","    \"Wie backe ich einen Apfelkuchen?\",\n","    \"Was muss ich über RxJS wissen?\",\n","    \"Wie progammiere ich mit LangGraph?\",\n","    \"In welchem Bundesland liegt München?\",\n","    \"Wie programmiere ich meinen Videorecorder?\",\n","    \"Wie viele Komponenten brauche ich mindestens für das Setup von Routing?\",\n","    \"Welcher Event signalisiert meiner Komponente, dass sie zerstört wird?\",\n","]\n","\n","# Store the question-answer pairs\n","results = [\n","    {\n","        \"question\": question,\n","        \"answer\": json.loads(\n","            llm_json_mode.invoke([SystemMessage(content=router_instructions)] + [HumanMessage(content=question)], config={\"callbacks\": [handler]}).content\n","        )\n","    }\n","    for question in questions\n","]\n","\n","# Print the results in a readable format\n","for result in results:\n","    print(f\"Question: {result['question']}\")\n","    print(f\"Answer: {result['answer']}\\n\")\n"]},{"cell_type":"markdown","metadata":{"id":"-ZS7BI-PPHCH"},"source":["### Prompt for grading search results from retriever/vector db"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jiFrRWzdZjpX"},"outputs":[],"source":["# retrieval grader - is the content from vectorestore a good fit for the question?\n","\n","# Doc grader instructions\n","doc_grader_instructions = \"\"\"You are a grader assessing carefully and objectively the relevance of a retrieved document to a user question.\n","If the document contains keywords or semantic meaning related to the question, grade it as relevant.\"\"\"\n","\n","# Grader prompt\n","doc_grader_prompt = \"\"\"Here is the retrieved document: \\n\\n {document} \\n\\n Here is the user question: \\n\\n {question}.\n","Return JSON with single key, binary_score, that is 'yes' or 'no' score to indicate whether the document contains at least some information that is relevant to the question.\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xsAP_XzvPqVf"},"outputs":[],"source":["# test the grader prompt\n","questions = [\n","    \"Wie progammiere ich mit LangGraph?\",\n","    \"Was ist besser signal inputs oder inputs()?\",\n","    \"Welche API nutzt der HttpClient standardmässig?\"\n","]\n","\n","# Process each question\n","for question in questions:\n","    # Retrieve documents based on the question\n","    docs = wiki_retriever.invoke(question)\n","    doc_txt = docs[0].page_content\n","\n","    # Format the prompt using the retrieved document\n","    doc_grader_prompt_formatted = doc_grader_prompt.format(\n","        document=doc_txt, question=question\n","    )\n","\n","    # Invoke the LLM with the formatted prompt\n","    result = llm_json_mode.invoke(\n","        [SystemMessage(content=doc_grader_instructions)] + [HumanMessage(content=doc_grader_prompt_formatted)]\n","    )\n","\n","    # Extract and print the results\n","    print(f\"Question: {question}\")\n","    print(f\"Document Title: {docs[0].metadata['title']}\")\n","    print(f\"Document URL: {docs[0].metadata['link']}\")\n","    print(f\"Document Content: {doc_txt[:150]}\")\n","    print(f\"Relevance: {json.loads(result.content)['binary_score']}\\n\")\n"]},{"cell_type":"markdown","metadata":{"id":"EZ1bz8lTRBye"},"source":["### Setup Tavily search client"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TS1-t8FbRFt1"},"outputs":[],"source":["# Tavily playground: https://app.tavily.com/playground\n","# Instantiating your TavilyClient\n","from tavily import TavilyClient\n","search_client = TavilyClient(api_key=userdata.get('TAVILY_API_KEY'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7k0bGO36UPkI"},"outputs":[],"source":["# Test open web search\n","from langchain.schema import Document\n","results = search_client.search(\"Wer ist Kimi Räikkönen\", search_depth=\"advanced\", max_results=2)\n","\n","# List to store the generated Document objects\n","documents = []\n","\n","# Iterate over each entry in the feed\n","for entry in results[\"results\"]:\n","    # Extract the page content\n","    page_content = entry.get('content', 'No content')\n","\n","    # Extract metadata\n","    metadata = {\n","        \"title\": entry.get('title', 'No Title'),\n","        \"link\": entry.get('url', 'No Link'),\n","        \"score\": entry.get('score', '0'),\n","    }\n","\n","    # Create a Document object for this entry\n","    document = Document(page_content=page_content, metadata=metadata)\n","\n","    # Append the document to the list\n","    documents.append(document)\n","\n","# Output results\n","for doc in documents:\n","    # Extract metadata and page content\n","    title = doc.metadata.get('title', 'N/A')\n","    link = doc.metadata.get('link', 'N/A')\n","    score = doc.metadata.get('score', 'N/A')\n","    content = doc.page_content[:150]  # Limit content for readability\n","\n","    # Print in a readable format\n","    print(f\"Title: {title}\")\n","    print(f\"Link: {link}\")\n","    print(f\"Score: {score}\")\n","    print(f\"Content: {content}\")\n","    print(\"-\" * 50 )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hsAKfag0Wq8T"},"outputs":[],"source":["# Test open web search with domain filter\n","from langchain.schema import Document\n","results = search_client.search(\"Wie viele Komponenten brauche ich mindestens für das Setup von Routing?\", search_depth=\"advanced\", max_results=2, include_domains=[\"angular.dev\"], include_raw_content=True)\n","\n","# List to store the generated Document objects\n","documents = []\n","\n","# Iterate over each entry in the feed\n","for entry in results[\"results\"]:\n","    # Extract the page content\n","    page_content = entry.get('raw_content', 'No content')\n","\n","    # Extract metadata\n","    metadata = {\n","        \"title\": entry.get('title', 'No Title'),\n","        \"link\": entry.get('url', 'No Link'),\n","        \"score\": entry.get('score', '0'),\n","    }\n","\n","    # Create a Document object for this entry\n","    document = Document(page_content=page_content, metadata=metadata)\n","\n","    # Append the document to the list\n","    documents.append(document)\n","\n","# Output results\n","for doc in documents:\n","    # Extract metadata and page content\n","    title = doc.metadata.get('title', 'N/A')\n","    link = doc.metadata.get('link', 'N/A')\n","    score = doc.metadata.get('score', 'N/A')\n","    content = doc.page_content[:150]  # Limit content for readability\n","\n","    # Print in a readable format\n","    print(f\"Title: {title}\")\n","    print(f\"Link: {link}\")\n","    print(f\"Score: {score}\")\n","    print(f\"Content: {content}\")\n","    print(\"-\" * 50 )"]},{"cell_type":"markdown","metadata":{"id":"xbuD7l0FvrgM"},"source":["## Setup Graph"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TVFhgvpATncL"},"outputs":[],"source":["from enum import Enum\n","class TRUSTLEVEL(Enum):\n","    TRUSTED_WIKI = 1\n","    TRUSTED_WEB = 2\n","    UNTRUSTED = 3"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ywyPa2HLvueZ"},"outputs":[],"source":["from typing_extensions import TypedDict\n","from typing import List\n","\n","class GraphState(TypedDict):\n","    \"\"\"\n","    Graph state is a dictionary that contains information we want to propagate to, and modify in, each graph node.\n","    \"\"\"\n","    question : str # User question\n","    generation : str # LLM generation\n","    answer_grade : str # Retrieved docs good for generation relevant/not_relevant\n","    documents : List[str] # List of retrieved documents\n","    trustability : TRUSTLEVEL # Trustability of source as enum"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"P2OUbm7lv5_o"},"outputs":[],"source":["from langchain_core.messages import HumanMessage\n","from langgraph.graph import START, END\n","\n","### Helper function\n","# Post-processing\n","def format_docs(docs):\n","    return \"\\n\\n\".join(doc.page_content for doc in docs)\n","\n","### Nodes\n","def retrieve(state):\n","    \"\"\"\n","    Retrieve documents from vectorstore\n","\n","    Args:\n","        state (dict): The current graph state\n","\n","    Returns:\n","        state (dict): New key added to state, documents, that contains retrieved documents\n","    \"\"\"\n","    print(\"---RETRIEVE---\")\n","    question = state[\"question\"]\n","\n","    # Write retrieved documents to documents key in state\n","    documents = wiki_retriever.invoke(question)\n","    return {\"documents\": documents, \"trustability\": TRUSTLEVEL.TRUSTED_WIKI}\n","\n","def grade(state):\n","    \"\"\"\n","    Grade retrieved documents\n","\n","    Args:\n","        state (dict): The current graph state\n","\n","    Returns:\n","        state (dict): New key added to state, answer_grade, that contains grade as relevant or not_relevant\n","    \"\"\"\n","    print(\"---GRADE---\")\n","    question = state[\"question\"]\n","    documents = state[\"documents\"]\n","\n","    # Doc grader instructions\n","    doc_grader_instructions = \"\"\"You are a grader assessing carefully and objectively the relevance of a retrieved document to a user question.\n","\n","    If the document contains keywords or semantic meaning related to the question, grade it as relevant.\"\"\"\n","\n","    # Grader prompt\n","    doc_grader_prompt = \"\"\"Here is the retrieved document: \\n\\n {document} \\n\\n Here is the user question: \\n\\n {question}.\n","\n","    Return JSON with single key, binary_score, that is 'relevant' or 'not_relevant' score to indicate whether the document contains at least some information that is relevant to the question.\"\"\"\n","\n","    # Prepare prompt and run grader\n","    doc_grader_prompt_formatted = doc_grader_prompt.format(document=documents[0].page_content, question=question)\n","    result = llm_json_mode.invoke(\n","        [SystemMessage(content=doc_grader_instructions)]\n","        + [HumanMessage(content=doc_grader_prompt_formatted)]\n","    )\n","    return {\"answer_grade\": json.loads(result.content)['binary_score']}\n","\n","def web_search_angular(state):\n","    \"\"\"\n","    Run web search for Angular content on angular.dev\n","\n","    Args:\n","        state (dict): The current graph state\n","\n","    Returns:\n","        state (dict): New key added to state, documents, that contains retrieved documents\n","    \"\"\"\n","    print(\"---WEB SEARCH ANGULAR---\")\n","    question = state[\"question\"]\n","    # Instantiating your TavilyClient\n","    from tavily import TavilyClient\n","    search_client = TavilyClient(api_key=userdata.get('TAVILY_API_KEY'))\n","\n","    # Run open web search\n","    from langchain.schema import Document\n","    results = search_client.search(question, search_depth=\"advanced\", max_results=2, include_domains=[\"angular.dev\"], include_raw_content=True)\n","\n","    # List to store the generated Document objects\n","    documents = []\n","\n","    # Iterate over each entry in the feed\n","    for entry in results[\"results\"]:\n","        # Extract the page content: prefer raw_content, fall back to content, and use default if both are empty\n","        page_content = entry.get('raw_content') or entry.get('content') or \"No content found\"\n","\n","        # Extract metadata\n","        metadata = {\n","            \"title\": entry.get('title', 'No Title'),\n","            \"link\": entry.get('url', 'No Link'),\n","            \"score\": entry.get('score', '0'),\n","        }\n","\n","        # Create a Document object for this entry\n","        document = Document(page_content=page_content, metadata=metadata)\n","\n","        # Append the document to the list\n","        documents.append(document)\n","\n","\n","    # Write retrieved documents to documents key in state\n","\n","    return{\"documents\": documents, \"trustability\": TRUSTLEVEL.TRUSTED_WEB}\n","\n","def web_search_full(state):\n","    \"\"\"\n","    Run web search for any content for given question\n","\n","    Args:\n","        state (dict): The current graph state\n","\n","    Returns:\n","        state (dict): New key added to state, documents, that contains retrieved documents\n","    \"\"\"\n","    print(\"---WEB SEARCH FULL---\")\n","    question = state[\"question\"]\n","    # Instantiating your TavilyClient\n","    from tavily import TavilyClient\n","    search_client = TavilyClient(api_key=userdata.get('TAVILY_API_KEY'))\n","\n","    # Run open web search\n","    from langchain.schema import Document\n","    results = search_client.search(question, search_depth=\"advanced\", max_results=2)\n","\n","    # List to store the generated Document objects\n","    documents = []\n","\n","    # Iterate over each entry in the feed\n","    for entry in results[\"results\"]:\n","        # Extract the page content\n","        page_content = entry.get('content', 'No content')\n","\n","        # Extract metadata\n","        metadata = {\n","            \"title\": entry.get('title', 'No Title'),\n","            \"link\": entry.get('url', 'No Link'),\n","            \"score\": entry.get('score', '0'),\n","        }\n","\n","        # Create a Document object for this entry\n","        document = Document(page_content=page_content, metadata=metadata)\n","\n","        # Append the document to the list\n","        documents.append(document)\n","\n","\n","    # Write retrieved documents to documents key in state\n","\n","    return{\"documents\": documents, \"trustability\": TRUSTLEVEL.UNTRUSTED}\n","\n","def generate(state):\n","    \"\"\"\n","    Generate answer using RAG on retrieved documents\n","\n","    Args:\n","        state (dict): The current graph state\n","\n","    Returns:\n","        state (dict): New key added to state, generation, that contains LLM generation\n","    \"\"\"\n","    print(\"---GENERATE---\")\n","    question = state[\"question\"]\n","    documents = state[\"documents\"]\n","\n","    # define answer prompt\n","    prompt_template = \"\"\"You are an assistant for question-answering tasks at ACME GmbH.\n","      Think carefully about the context.\n","      Just say 'Diese Frage kann ich nicht beantworten' if there is not enough or no context given.\n","      Provide an answer to the user question using only the given context.\n","      Use three sentences maximum and keep the answer concise.\n","      If the context mentions ACME guidelines, try to include it in the answer.\n","      Here is the context to use to answer the question:\n","\n","      {context}\n","\n","      Now, review the user question:\n","\n","      {question}\n","\n","      Write the answer in German. Don't output an English translation.\n","\n","      Answer:\"\"\"\n","\n","    # RAG generation\n","    docs_txt = format_docs(documents)\n","    rag_prompt_formatted = prompt_template.format(context=docs_txt, question=question)\n","    generation = llm.invoke([HumanMessage(content=rag_prompt_formatted)])\n","    return {\"generation\": generation}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iGNuYXT2ViVk"},"outputs":[],"source":["### Conditional nodes\n","def route_question(state):\n","    \"\"\"\n","    Route question to web search or RAG\n","\n","    Args:\n","        state (dict): The current graph state\n","\n","    Returns:\n","        str: Next node to call\n","    \"\"\"\n","\n","    print(\"---ROUTE QUESTION---\")\n","    route_question = llm_json_mode.invoke(\n","        [SystemMessage(content=router_instructions)]\n","        + [HumanMessage(content=state[\"question\"])]\n","    )\n","    source = json.loads(route_question.content)[\"datasource\"]\n","    if source == \"websearch\":\n","        print(\"---ROUTE QUESTION TO WEB_SEARCH_FULL---\")\n","        return \"websearch\"\n","    elif source == \"vectorstore\":\n","        print(\"---ROUTE QUESTION TO RETRIEVER---\")\n","        return \"vectorstore\"\n","\n","def decide_retriever_ok(state):\n","    \"\"\"\n","    Determines whether retrieved content is good to generate an answer, or run web search\n","\n","    Args:\n","        state (dict): The current graph state\n","\n","    Returns:\n","        str: Binary decision for next node to call\n","    \"\"\"\n","    print(\"---DECIDE RETRIEVER OK---\")\n","    answer_grade = state[\"answer_grade\"]\n","\n","    if answer_grade.lower() == \"not_relevant\":\n","        print(\n","            \"---DECISION: NOT ALL DOCUMENTS ARE RELEVANT TO QUESTION, INCLUDE WEB SEARCH---\"\n","        )\n","        return \"websearch\"\n","    else:\n","        # We have relevant documents, so generate answer\n","        print(\"---DECISION: GENERATE---\")\n","        return \"generate\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K9I2Ugcz4sTq"},"outputs":[],"source":["from langgraph.graph import StateGraph\n","from IPython.display import Image, display\n","\n","workflow = StateGraph(GraphState)\n","\n","# Define the nodes\n","workflow.add_node(\"retrieve\", retrieve) # retrieve\n","workflow.add_node(\"generate\", generate) # generate\n","workflow.add_node(\"grade\", grade) # grade\n","workflow.add_node(\"web_search_angular\", web_search_angular) # websearch angular.dev\n","workflow.add_node(\"web_search_full\", web_search_full) # full websearch\n","\n","# Define the edges\n","workflow.set_conditional_entry_point(\n","    route_question,\n","    {\n","        \"websearch\": \"web_search_full\",\n","        \"vectorstore\": \"retrieve\",\n","    },\n",")\n","workflow.add_edge(\"web_search_full\", \"generate\")\n","workflow.add_edge(\"retrieve\", \"grade\")\n","workflow.add_conditional_edges(\n","    \"grade\",\n","    decide_retriever_ok,\n","    {\n","        \"websearch\": \"web_search_angular\",\n","        \"generate\": \"generate\",\n","    },\n",")\n","workflow.add_edge(\"web_search_angular\", \"generate\")\n","workflow.add_edge(\"generate\", END)\n","\n","# Compile\n","graph = workflow.compile()\n","display(Image(graph.get_graph().draw_mermaid_png()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AkZH23HEWBWE"},"outputs":[],"source":["# format result output\n","def print_graph_result(result):\n","    # Print question, generated answer, trust level, answer grade\n","    print(f\"Question: {result.get('question', 'N/A')}\")\n","    print(f\"Generation: {result.get('generation').content if result.get('generation') else 'N/A'}\")\n","    print(f\"Trustability: {result.get('trustability', 'N/A')}\")\n","    print(f\"Answer Grade: {result.get('answer_grade', 'N/A')}\\n\")\n","\n","    # Print additional metadata from the generation\n","    generation = result.get('generation')\n","    if generation:\n","        response_metadata = generation.response_metadata\n","        print(\"Response Metadata:\")\n","        for key, value in response_metadata.items():\n","            print(f\"  {key}: {value}\")\n","        print()\n","\n","    # Print documents\n","    documents = result.get('documents', [])\n","    for index, doc in enumerate(documents):\n","        print(f\"Document {index + 1}:\")\n","        metadata = doc.metadata\n","        for key, value in metadata.items():\n","            print(f\"  {key}: {value}\")\n","        print(f\"  Content: {doc.page_content[:150]}...\")  # Limit content to 150 characters"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZSbDrhT2UWp1"},"outputs":[],"source":["result = graph.invoke({\"question\": \"Was ist besser signal inputs oder inputs()?\"}, config={\"callbacks\": [handler]})\n","print_graph_result(result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H03_giSX-Rzj"},"outputs":[],"source":["result = graph.invoke({\"question\": \"Wie backe ich einen Apfelkuchen?\"}, config={\"callbacks\": [handler]})\n","print_graph_result(result)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c4RWuU90h9B3"},"outputs":[],"source":["result = graph.invoke({\"question\": \"Wie viele Komponenten brauche ich mindestens für das Setup von Routing?\"}, config={\"callbacks\": [handler]})\n","print_graph_result(result)"]}],"metadata":{"colab":{"collapsed_sections":["lJy5wsZuHvXz"],"provenance":[{"file_id":"1o3PbdmzBt_Hw47glh0ZMDWWtkpfle2Q8","timestamp":1732283358404},{"file_id":"1RJY2Iw1syFXQqskaW-trGSAK6TDw49os","timestamp":1727958136285},{"file_id":"1YW8tGacff05Ie7sSAtF7f6ItjA-l0R-B","timestamp":1727765579943},{"file_id":"14hP4Ld97QlJdWTBaHU2I2e0ZzMUw-WF-","timestamp":1727599529874},{"file_id":"1D2QriJvCzNUdU__n7VEWxebiYVnF1GwO","timestamp":1704649306386}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}