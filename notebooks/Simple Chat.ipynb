{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vigmlask6ys0"
   },
   "source": [
    "<img src=\"https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png\" srcset=\"https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png/w_130 130w, https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png/w_260 260w, https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png/w_390 390w, https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png/w_520 520w, https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png/w_650 650w, https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png/w_780 780w, https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png/w_910 910w, https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png/w_1040 1040w, https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png/w_1170 1170w, https://71022.cdn.cke-cs.com/RructTCFEHceQFc13ldy/images/6dbe93b28dbb43fbc9d50623b68a675a1fedd7608af93b46.png/w_1290 1290w\" sizes=\"100vw\" width=\"1290\">\n",
    "<p style='margin-top: 1rem; margin-bottom: 1rem;'>Developed by Marco Frodl, Principal Consultant for Generative AI @ <a href='https://go.mfr.one/tt-en' _target='blank'>Thinktecture AG</a> -- More about me on my <a href='https://go.mfr.one/marcofrodl-en' _target='blank'>profile page</a></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJy5wsZuHvXz"
   },
   "source": [
    "## Prettify Colab Notebook outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xOA06-GRFTeK"
   },
   "outputs": [],
   "source": [
    "from IPython.display import HTML, display\n",
    "\n",
    "def set_css():\n",
    "  display(HTML('''\n",
    "  <style>\n",
    "    pre {\n",
    "        white-space: pre-wrap;\n",
    "    }\n",
    "  </style>\n",
    "  '''))\n",
    "get_ipython().events.register('pre_run_cell', set_css)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XUC6k6SzJE5a"
   },
   "source": [
    "### Set API keys via Colab Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 323
    },
    "executionInfo": {
     "elapsed": 7677,
     "status": "error",
     "timestamp": 1732199016469,
     "user": {
      "displayName": "Sebastian Gingter",
      "userId": "03749202599653192620"
     },
     "user_tz": -60
    },
    "id": "7Aj0mSvhyJI5",
    "outputId": "992db916-7b00-4d8d-8d5f-2ca312d7500e"
   },
   "outputs": [],
   "source": [
    "# import Colab Secrets userdata module\n",
    "from google.colab import userdata\n",
    "\n",
    "# set OpenAI API key\n",
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
    "\n",
    "# set Langfuse API keys\n",
    "os.environ[\"LANGFUSE_PUBLIC_KEY\"] = userdata.get('LANGFUSE_PUBLIC_KEY')\n",
    "os.environ[\"LANGFUSE_SECRET_KEY\"] = userdata.get('LANGFUSE_SECRET_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6jh11ajrHjFC"
   },
   "source": [
    "## Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pfrfCqwVVG_Y"
   },
   "outputs": [],
   "source": [
    "!pip -q install protobuf==3.20.3 langchain==0.3.1 langchain-openai==0.2.1 langchain-mistralai==0.2.0 langfuse==2.51.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wsfvcpLnQyw_"
   },
   "outputs": [],
   "source": [
    "# let's check the imported versions of the most important libraries\n",
    "!pip show langchain\n",
    "!pip show langchain-openai\n",
    "!pip show langchain-mistralai\n",
    "!pip show langfuse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vOP866IZG87z"
   },
   "source": [
    "## Prepare Langfuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gdG06ivx5xV1"
   },
   "outputs": [],
   "source": [
    "# prepare Langfuse as debugging and tracing framework for our Generative AI application - never develop GenAI apps without that!\n",
    "from langfuse.callback import CallbackHandler\n",
    "handler = CallbackHandler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PflymEKFgBBl"
   },
   "source": [
    "## Prepare LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DaIaBsel8Cf-"
   },
   "source": [
    "### OpenAI LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 285,
     "status": "ok",
     "timestamp": 1727717429067,
     "user": {
      "displayName": "Marco Frodl",
      "userId": "15184785398550850138"
     },
     "user_tz": -120
    },
    "id": "CQCcOM-S8Jmp",
    "outputId": "0ea4f35f-801c-437f-af3a-cae39a7f5a41"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm_openai = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0, max_tokens=1000) # gpt-4o-mini, gpt-4o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QZilCIWJ7_PX"
   },
   "source": [
    "### Mistral AI LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "executionInfo": {
     "elapsed": 1956,
     "status": "ok",
     "timestamp": 1727717436357,
     "user": {
      "displayName": "Marco Frodl",
      "userId": "15184785398550850138"
     },
     "user_tz": -120
    },
    "id": "YJ3o8h7OgBBm",
    "outputId": "30bbd4dc-ef34-4de1-82cc-72bbe73b53dc"
   },
   "outputs": [],
   "source": [
    "from langchain_mistralai import ChatMistralAI\n",
    "llm_mistralai = ChatMistralAI(mistral_api_key=userdata.get('MISTRAL_API_KEY'), temperature=0, max_tokens=1000, model=\"open-mistral-nemo-2407\") # open-mistral-nemo-2407, mistral-large-2407, mistral-small-2409"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rn4nosl8fMCy"
   },
   "source": [
    "## Simple LLM chat call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cXO6J-UJxedm"
   },
   "outputs": [],
   "source": [
    "# pure as water\n",
    "result = llm_openai.invoke(\"Which is the best generative ai large language model in the world?\")\n",
    "print(\"--- OpenAI ---\")\n",
    "print(result.content)\n",
    "print(result.response_metadata)\n",
    "\n",
    "result = llm_mistralai.invoke(\"Which is the best generative ai large language model in the world?\")\n",
    "print(\"--- MistralAI ---\")\n",
    "print(result.content)\n",
    "print(result.response_metadata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tFi-6yoeCu6D"
   },
   "source": [
    "## Chat with Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xO8T25nu_uEj"
   },
   "outputs": [],
   "source": [
    "# little bit more context about who is talking\n",
    "from langchain_core.messages import HumanMessage\n",
    "result = llm_openai.invoke([HumanMessage(content=\"Kennst du meinen Namen?\", name=\"Marco\")])\n",
    "print(\"--- OpenAI ---\")\n",
    "print(result.content)\n",
    "print(result.response_metadata)\n",
    "\n",
    "result = llm_mistralai.invoke([HumanMessage(content=\"Kennst du meinen Namen?\", name=\"Marco\")]) # name is not supported by ChatMistralAI\n",
    "print(\"--- MistralAI ---\")\n",
    "print(result.content)\n",
    "print(result.response_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1noY3GTC0_W"
   },
   "source": [
    "## Chat with Tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-rkMVTf3BoSQ"
   },
   "outputs": [],
   "source": [
    "# lets optimize and trace whats happening\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "messages = [SystemMessage(content=\"Du bist eine AI, die den User Duzt und gerne Wörter wie Bro, Digga und ich schwör verwendet. Der Name des Users ist Marco\")]\n",
    "messages.extend([HumanMessage(content=\"Kennst du meinen Namen?\")])\n",
    "\n",
    "result = llm_openai.invoke(messages,{\"callbacks\":[handler]})\n",
    "print(\"--- OpenAI ---\")\n",
    "print(result.content)\n",
    "print(result.response_metadata)\n",
    "\n",
    "result = llm_mistralai.invoke(messages,{\"callbacks\":[handler]})\n",
    "print(\"--- MistralAI ---\")\n",
    "print(result.content)\n",
    "print(result.response_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IKrY_r86C5_9"
   },
   "source": [
    "## Chat with history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iYCtPi4YDyR_"
   },
   "outputs": [],
   "source": [
    "# Dialog based chat\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage\n",
    "messages = [\n",
    "    SystemMessage(content=\"Du bist eine AI, die sich mit Popmusik auskennt und viele Fakten in Antworten einfliessen lässt. Du Duzt den User.\"),\n",
    "    HumanMessage(content=\"Genesis war ohne Peter Gabriel ein Schatten seiner selbt. Wer war danach der Sänger?\"),\n",
    "    AIMessage(content=\"Phil Collins - allerdings muss ich beim Thema Erfolg widersprechen!\"),\n",
    "    HumanMessage(content=\"Ok, warum?\")]\n",
    "\n",
    "result = llm_openai.invoke(messages,{\"callbacks\":[handler]})\n",
    "print(\"--- OpenAI ---\")\n",
    "print(result.content)\n",
    "print(result.response_metadata)\n",
    "\n",
    "result = llm_mistralai.invoke(messages,{\"callbacks\":[handler]})\n",
    "print(\"--- MistralAI ---\")\n",
    "print(result.content)\n",
    "print(result.response_metadata)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "lJy5wsZuHvXz"
   ],
   "provenance": [
    {
     "file_id": "17WvPo4UxyAsI0r1I3SxyfWRJPFI3H1Bc",
     "timestamp": 1732294764556
    },
    {
     "file_id": "14hP4Ld97QlJdWTBaHU2I2e0ZzMUw-WF-",
     "timestamp": 1721639508829
    },
    {
     "file_id": "1D2QriJvCzNUdU__n7VEWxebiYVnF1GwO",
     "timestamp": 1704649306386
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
